{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Object detection in KITTI-MOTS",
   "id": "abdb3cb274320aa5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define pretrained model name\n",
    "MODEL_NAME = \"microsoft/conditional-detr-resnet-50\"\n",
    "IMAGE_SIZE = 480"
   ],
   "id": "6a37db7b25ae4275"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the dataset",
   "id": "8de18e31df69ada3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Decode and extract the whole dataset into a JSON",
   "id": "554c86869f758951"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "def parse_kitti_mots_annotation(annotation_line):\n",
    "    \"\"\"\n",
    "    Parses a single KITTI-MOTS annotation line.\n",
    "    \"\"\"\n",
    "    fields = annotation_line.strip().split()\n",
    "\n",
    "    frame_id = int(fields[0])  # Frame number\n",
    "    object_id = int(fields[1])  # Example: 1005 (means class_id=1, instance_id=5)\n",
    "    class_id = int(fields[2])  # Extract class ID\n",
    "    instance_id = object_id % 1000  # Extract instance ID\n",
    "    height = int(fields[3])  # Image height\n",
    "    width = int(fields[4])  # Image width\n",
    "    rle_str = \" \".join(fields[5:])  # RLE encoding\n",
    "\n",
    "    return {\n",
    "        \"frame_id\": frame_id,\n",
    "        \"object_id\": object_id,\n",
    "        \"class_id\": class_id,\n",
    "        \"instance_id\": instance_id,\n",
    "        \"height\": height,\n",
    "        \"width\": width,\n",
    "        \"rle_str\": rle_str\n",
    "    }\n",
    "\n",
    "def decode_rle_and_get_bbox(rle_str, height, width):\n",
    "    \"\"\"\n",
    "    Decodes RLE and computes the bounding box.\n",
    "    \"\"\"\n",
    "    coco_rle = {\n",
    "        \"counts\": rle_str.encode(\"utf-8\"),\n",
    "        \"size\": [height, width]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        mask = maskUtils.decode(coco_rle)\n",
    "        y_indices, x_indices = np.where(mask > 0)\n",
    "        if len(y_indices) == 0 or len(x_indices) == 0:\n",
    "            return mask, None, 0\n",
    "\n",
    "        x_min, x_max = x_indices.min(), x_indices.max()\n",
    "        y_min, y_max = y_indices.min(), y_indices.max()\n",
    "        bbox = [float(x_min), float(y_min), float(x_max - x_min), float(y_max - y_min)]\n",
    "        area = int(mask.sum())\n",
    "\n",
    "        return mask, bbox, area\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding RLE: {e}\")\n",
    "        return None, None, 0\n",
    "\n",
    "def convert_kitti_mots_to_coco(annotation_folder, image_folder):\n",
    "    \"\"\"\n",
    "    Converts KITTI-MOTS annotations to COCO format.\n",
    "    \"\"\"\n",
    "    coco_data = []\n",
    "\n",
    "    for txt_file in sorted(os.listdir(annotation_folder)):\n",
    "        if not txt_file.endswith(\".txt\"):\n",
    "            continue\n",
    "\n",
    "        sequence_id = txt_file.split(\".\")[0]  # Example: '0000'\n",
    "        txt_path = os.path.join(annotation_folder, txt_file)\n",
    "        image_sequence_folder = os.path.join(image_folder, sequence_id)\n",
    "\n",
    "        image_files = sorted(os.listdir(image_sequence_folder))\n",
    "        image_data = {}\n",
    "\n",
    "        with open(txt_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                parsed = parse_kitti_mots_annotation(line)\n",
    "\n",
    "                # Ignore annotations where class_id is 10\n",
    "                if parsed[\"class_id\"] == 10:\n",
    "                    continue\n",
    "\n",
    "                frame_id = parsed[\"frame_id\"]\n",
    "\n",
    "                image_filename = f\"{frame_id:06d}.png\"\n",
    "                image_path = os.path.join(image_sequence_folder, image_filename)\n",
    "\n",
    "                # Modify image reference to include the sequence ID (folder name)\n",
    "                image_reference = os.path.join(sequence_id, image_filename)\n",
    "\n",
    "                # Try to load image dimensions\n",
    "                try:\n",
    "                    with Image.open(image_path) as img:\n",
    "                        width, height = img.size\n",
    "                except Exception:\n",
    "                    print(f\"Warning: Image {image_path} not found.\")\n",
    "                    width, height = parsed[\"width\"], parsed[\"height\"]\n",
    "\n",
    "                mask, bbox, area = decode_rle_and_get_bbox(parsed[\"rle_str\"], parsed[\"height\"], parsed[\"width\"])\n",
    "                if bbox is None:\n",
    "                    continue\n",
    "\n",
    "                if image_reference not in image_data:\n",
    "                    image_data[image_reference] = {\n",
    "                        \"image_id\": frame_id,  # Unique identifier\n",
    "                        \"image\": image_reference,  # Reference includes folder\n",
    "                        \"width\": width,\n",
    "                        \"height\": height,\n",
    "                        \"objects\": {\"id\": [], \"area\": [], \"bbox\": [], \"category\": []}\n",
    "                    }\n",
    "\n",
    "                image_data[image_reference][\"objects\"][\"id\"].append(parsed[\"instance_id\"])\n",
    "                image_data[image_reference][\"objects\"][\"area\"].append(area)\n",
    "                image_data[image_reference][\"objects\"][\"bbox\"].append(bbox)\n",
    "                image_data[image_reference][\"objects\"][\"category\"].append(parsed[\"class_id\"])\n",
    "\n",
    "        coco_data.extend(image_data.values())\n",
    "\n",
    "    return coco_data\n",
    "\n",
    "# Define paths\n",
    "annotation_folder = \"../data/KITTI-MOTS/instances_txt\"\n",
    "image_folder = \"../data/KITTI-MOTS/training/images\"\n",
    "\n",
    "# Convert to COCO format\n",
    "coco_annotations = convert_kitti_mots_to_coco(annotation_folder, image_folder)\n",
    "\n",
    "# Save as JSON\n",
    "with open(\"kitti_mots_coco.json\", \"w\") as json_file:\n",
    "    json.dump(coco_annotations, json_file, indent=4)\n",
    "\n",
    "# Print sample output\n",
    "print(json.dumps(coco_annotations[:2], indent=4))  # Print first 2 annotations"
   ],
   "id": "16274c7399137130"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Split Train and Test samples",
   "id": "d95180a4929934e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from datasets import Dataset, DatasetDict, Features, Value, Sequence, ClassLabel, Image\n",
    "\n",
    "def load_kitti_mots(annotation_file):\n",
    "    \"\"\"Cargar anotaciones COCO en JSON\"\"\"\n",
    "    with open(annotation_file, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def extract_class_mapping(coco_annotations):\n",
    "    \"\"\"\n",
    "    Extrae y remapea IDs de clase a un rango continuo,\n",
    "    mapeando 1 a 'car' y 2 a 'pedestrian'.\n",
    "    Solo se consideran anotaciones con clase 1 o 2.\n",
    "    \"\"\"\n",
    "    # Definir el mapeo fijo\n",
    "    fixed_mapping = {1: 0, 2: 1}\n",
    "    fixed_names = {1: \"car\", 2: \"pedestrian\"}\n",
    "    # Filtrar sólo anotaciones con clase 1 o 2\n",
    "    class_ids = sorted(set(\n",
    "        cat\n",
    "        for sample in coco_annotations\n",
    "        for cat in sample[\"objects\"][\"category\"]\n",
    "        if cat in fixed_mapping\n",
    "    ))\n",
    "    class_mapping = {cat: fixed_mapping[cat] for cat in class_ids}\n",
    "    class_names = [fixed_names[cat] for cat in class_ids]\n",
    "    return class_mapping, class_names\n",
    "\n",
    "def split_dataset_by_sequence(coco_annotations, train_ratio=0.8, val_ratio=0.2, seed=1337):\n",
    "    \"\"\"\n",
    "    Divide el dataset en train y validation basándose en secuencias.\n",
    "    Se asume que cada muestra tiene un campo \"image\" con un path que incluye la carpeta de secuencia.\n",
    "    \"\"\"\n",
    "    # Agrupar las anotaciones por secuencia (extraer la carpeta de la imagen)\n",
    "    sequences = {}\n",
    "    for sample in coco_annotations:\n",
    "        sequence_folder = os.path.dirname(sample[\"image\"])\n",
    "        if sequence_folder not in sequences:\n",
    "            sequences[sequence_folder] = []\n",
    "        sequences[sequence_folder].append(sample)\n",
    "\n",
    "    sequence_keys = list(sequences.keys())\n",
    "    random.seed(seed)\n",
    "    random.shuffle(sequence_keys)\n",
    "\n",
    "    total_sequences = len(sequence_keys)\n",
    "    train_count = int(total_sequences * train_ratio)\n",
    "\n",
    "    train_keys = sequence_keys[:train_count]\n",
    "    val_keys = sequence_keys[train_count:]\n",
    "\n",
    "    # Reconstruir los splits a partir de las secuencias\n",
    "    train_samples = [sample for key in train_keys for sample in sequences[key]]\n",
    "    val_samples = [sample for key in val_keys for sample in sequences[key]]\n",
    "\n",
    "    return {\n",
    "        \"train\": train_samples,\n",
    "        \"validation\": val_samples,\n",
    "    }\n",
    "\n",
    "def create_hf_dataset(coco_data, image_root, class_mapping, class_names):\n",
    "    \"\"\"\n",
    "    Convierte COCO a Hugging Face Dataset sin cargar la imagen en memoria.\n",
    "    Solo se incluyen objetos cuya categoría esté en el mapeo (1 o 2).\n",
    "    \"\"\"\n",
    "    formatted_data = []\n",
    "    for sample in coco_data:\n",
    "        image_path = os.path.join(image_root, sample[\"image\"])  # Guardamos la ruta de la imagen\n",
    "        object_list = []\n",
    "        for i in range(len(sample[\"objects\"][\"id\"])):\n",
    "            original_cat = sample[\"objects\"][\"category\"][i]\n",
    "            if original_cat not in class_mapping:\n",
    "                continue\n",
    "            bbox = list(map(float, sample[\"objects\"][\"bbox\"][i]))\n",
    "            object_list.append({\n",
    "                \"id\": int(sample[\"objects\"][\"id\"][i]),\n",
    "                \"area\": int(sample[\"objects\"][\"area\"][i]),\n",
    "                \"bbox\": bbox,\n",
    "                \"category\": class_mapping[original_cat]  # Re-mapeamos a índice 0 o 1\n",
    "            })\n",
    "\n",
    "        formatted_data.append({\n",
    "            \"image_id\": int(sample[\"image_id\"]),\n",
    "            \"image\": image_path,\n",
    "            \"width\": int(sample[\"width\"]),\n",
    "            \"height\": int(sample[\"height\"]),\n",
    "            \"objects\": object_list\n",
    "        })\n",
    "\n",
    "    # Definir esquema de características (sin cargar la imagen)\n",
    "    features = Features({\n",
    "        \"image_id\": Value(\"int64\"),\n",
    "        \"image\": Value(\"string\"),\n",
    "        \"width\": Value(\"int32\"),\n",
    "        \"height\": Value(\"int32\"),\n",
    "        \"objects\": Sequence({\n",
    "            \"id\": Value(\"int64\"),\n",
    "            \"area\": Value(\"int64\"),\n",
    "            \"bbox\": Sequence(Value(\"float32\"), length=4),\n",
    "            \"category\": ClassLabel(names=class_names)\n",
    "        })\n",
    "    })\n",
    "\n",
    "    return Dataset.from_list(formatted_data, features=features)\n",
    "\n",
    "# Cargar dataset\n",
    "annotation_file = \"kitti_mots_coco.json\"\n",
    "image_root = \"../data/KITTI-MOTS/training/images\"\n",
    "coco_annotations = load_kitti_mots(annotation_file)\n",
    "\n",
    "# Extraer mapeo de clases (solo se consideran clases 1 y 2)\n",
    "class_mapping, class_names = extract_class_mapping(coco_annotations)\n",
    "print(\"Class mapping:\", class_mapping)\n",
    "print(\"Class names:\", class_names)\n",
    "\n",
    "# Dividir dataset por secuencias (80% train, 20% validation)\n",
    "dataset_splits = split_dataset_by_sequence(coco_annotations)\n",
    "\n",
    "# Convertir a Hugging Face `datasets.Dataset`\n",
    "hf_dataset = DatasetDict({\n",
    "    \"train\": create_hf_dataset(dataset_splits[\"train\"], image_root, class_mapping, class_names),\n",
    "    \"validation\": create_hf_dataset(dataset_splits[\"validation\"], image_root, class_mapping, class_names),\n",
    "})\n",
    "\n",
    "# Guardar dataset\n",
    "hf_dataset.save_to_disk(\"kitti_mots_hf\")\n",
    "\n",
    "# Imprimir estructura\n",
    "print(hf_dataset[\"train\"].features)\n",
    "\n"
   ],
   "id": "50ed0a954303a499"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "hf_dataset[\"train\"][1]['objects']",
   "id": "742eec30d648ec01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Draw an example",
   "id": "1ab2e63d6b06f419"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import io\n",
    "\n",
    "image = hf_dataset[\"train\"][1][\"image\"]\n",
    "annotations = hf_dataset[\"train\"][1][\"objects\"]\n",
    "image_open = Image.open(image).convert(\"RGB\")\n",
    "draw = ImageDraw.Draw(image_open)\n",
    "\n",
    "categories = hf_dataset[\"train\"].features[\"objects\"].feature[\"category\"].names\n",
    "print(categories)\n",
    "\n",
    "id2label = {index: x for index, x in enumerate(categories, start=0)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "\n",
    "for i in range(len(annotations[\"id\"])):\n",
    "    box = annotations[\"bbox\"][i]\n",
    "    class_idx = annotations[\"category\"][i]\n",
    "    x, y, w, h = tuple(box)\n",
    "\n",
    "    # Check if coordinates are normalized or not\n",
    "    if max(box) > 1.0:\n",
    "        # Coordinates are un-normalized, no need to re-scale them\n",
    "        x1, y1 = int(x), int(y)\n",
    "        x2, y2 = int(x + w), int(y + h)\n",
    "    else:\n",
    "        # Coordinates are normalized, re-scale them\n",
    "        x1 = int(x * hf_dataset[\"train\"][1][\"width\"])\n",
    "        y1 = int(y * hf_dataset[\"train\"][1][\"height\"])\n",
    "        x2 = int((x + w) * hf_dataset[\"train\"][1][\"width\"])\n",
    "        y2 = int((y + h) * hf_dataset[\"train\"][1][\"height\"])\n",
    "    draw.rectangle((x1, y1, x2, y2), outline=\"red\", width=1)\n",
    "    draw.text((x1, y1), str(id2label[class_idx]), fill=\"red\")\n",
    "\n",
    "image_open"
   ],
   "id": "f9822893682b3731"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Infer and Evaluate DETR without finetuning",
   "id": "fc2ea5ad5fbbe29e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from transformers import AutoFeatureExtractor, ConditionalDetrForObjectDetection\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Para la evaluación con pycocotools\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import copy\n",
    "\n",
    "# ------------------------------\n",
    "# Cargar modelo y feature extractor\n",
    "model_name = \"microsoft/conditional-detr-resnet-50\"\n",
    "model = ConditionalDetrForObjectDetection.from_pretrained(model_name)\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# ------------------------------\n",
    "# Función para obtener predicciones para una imagen individual\n",
    "def get_predictions(sample, threshold=0.2):\n",
    "    image = Image.open(sample[\"image\"]).convert(\"RGB\")\n",
    "\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # target_sizes espera (height, width)\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = feature_extractor.post_process_object_detection(\n",
    "        outputs, target_sizes=target_sizes, threshold=threshold\n",
    "    )[0]\n",
    "\n",
    "    predictions = []\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        x_min, y_min, x_max, y_max = box.tolist()\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "        predictions.append({\n",
    "            \"image_id\": sample[\"image_id\"],\n",
    "            \"category_id\": int(label),  # Debe coincidir con el ID de la categoría\n",
    "            \"bbox\": [x_min, y_min, width, height],\n",
    "            \"score\": float(score)\n",
    "        })\n",
    "    return predictions\n",
    "\n",
    "# ------------------------------\n",
    "# Ejemplo de dataset con las claves necesarias:\n",
    "# hf_dataset[\"validation\"] -> lista/diccionario con:\n",
    "# {\n",
    "#   \"image_id\": ...,\n",
    "#   \"image\": \"ruta_a_la_imagen.jpg\",\n",
    "#   \"width\": ...,\n",
    "#   \"height\": ...,\n",
    "#   \"objects\": {\n",
    "#       \"id\": [...],\n",
    "#       \"category\": [...],\n",
    "#       \"bbox\": [...],  # en formato [x, y, width, height]\n",
    "#       \"area\": [...]\n",
    "#   }\n",
    "# }\n",
    "\n",
    "all_predictions = []\n",
    "images = []\n",
    "annotations = []\n",
    "annotation_id = 1\n",
    "\n",
    "# Define las categorías (IDs consistentes con tus predicciones)\n",
    "categories = [\n",
    "    {\"id\": 0, \"name\": \"car\"},\n",
    "    {\"id\": 1, \"name\": \"person\"}\n",
    "]\n",
    "\n",
    "# Recorremos el conjunto de validación para construir anotaciones y predicciones\n",
    "for sample in hf_dataset[\"validation\"]:\n",
    "    images.append({\n",
    "        \"id\": sample[\"image_id\"],\n",
    "        \"width\": sample[\"width\"],\n",
    "        \"height\": sample[\"height\"]\n",
    "    })\n",
    "\n",
    "    # Agregar anotaciones ground truth\n",
    "    for i in range(len(sample[\"objects\"][\"id\"])):\n",
    "        annotations.append({\n",
    "            \"id\": annotation_id,\n",
    "            \"image_id\": sample[\"image_id\"],\n",
    "            \"category_id\": sample[\"objects\"][\"category\"][i],\n",
    "            \"bbox\": sample[\"objects\"][\"bbox\"][i],\n",
    "            \"area\": sample[\"objects\"][\"area\"][i],\n",
    "            \"iscrowd\": 0\n",
    "        })\n",
    "        annotation_id += 1\n",
    "\n",
    "    # Obtener predicciones\n",
    "    preds = get_predictions(sample, threshold=0.1)\n",
    "    all_predictions.extend(preds)\n",
    "\n",
    "# ------------------------------\n",
    "# Construir el diccionario de ground truth en formato COCO\n",
    "gt_dict = {\n",
    "    \"images\": images,\n",
    "    \"annotations\": annotations,\n",
    "    \"categories\": categories\n",
    "}\n",
    "\n",
    "# Crear el objeto COCO de ground truth\n",
    "coco_gt = COCO()\n",
    "coco_gt.dataset = gt_dict\n",
    "coco_gt.createIndex()\n",
    "\n",
    "# Cargar las detecciones en formato COCO\n",
    "coco_dt = coco_gt.loadRes(all_predictions)\n",
    "\n",
    "# ------------------------------\n",
    "# Función para obtener métricas mAP@0.5:0.95 y mAP@0.5\n",
    "def evaluate_coco(coco_gt, coco_dt, catIds=None):\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    # Filtra por categorías si se especifican (car -> [0], person -> [1])\n",
    "    if catIds is not None:\n",
    "        coco_eval.params.catIds = catIds\n",
    "\n",
    "    # Realiza la evaluación con el rango por defecto de IoU (0.5 a 0.95)\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    # coco_eval.stats es un array con 12 índices:\n",
    "    # stats[0] = AP (IoU=0.5:0.95) - área: all\n",
    "    # stats[1] = AP (IoU=0.5)      - área: all\n",
    "    # stats[2] = AP (IoU=0.75)     - área: all\n",
    "    # ...\n",
    "    mAP_50_95 = coco_eval.stats[0]  # AP promedio en IoU de 0.5 a 0.95\n",
    "    mAP_50 = coco_eval.stats[1]     # AP a IoU=0.5\n",
    "    return mAP_50_95, mAP_50\n",
    "\n",
    "# ------------------------------\n",
    "# 1) Métricas para todas las categorías juntas\n",
    "mAP_50_95_all, mAP_50_all = evaluate_coco(coco_gt, coco_dt, catIds=None)\n",
    "print(f\"mAP@0.5:0.95 (All): {mAP_50_95_all:.3f}\")\n",
    "print(f\"mAP@0.5 (All): {mAP_50_all:.3f}\")\n",
    "\n",
    "# 2) Métricas para 'car' (ID=0)\n",
    "mAP_50_95_car, mAP_50_car = evaluate_coco(coco_gt, coco_dt, catIds=[0])\n",
    "print(f\"mAP@0.5:0.95 (Car): {mAP_50_95_car:.3f}\")\n",
    "print(f\"mAP@0.5 (Car): {mAP_50_car:.3f}\")\n",
    "\n",
    "# 3) Métricas para 'person' (ID=1)\n",
    "mAP_50_95_person, mAP_50_person = evaluate_coco(coco_gt, coco_dt, catIds=[1])\n",
    "print(f\"mAP@0.5:0.95 (Person): {mAP_50_95_person:.3f}\")\n",
    "print(f\"mAP@0.5 (Person): {mAP_50_person:.3f}\")"
   ],
   "id": "c977a3bfba6110d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "MAX_SIZE = IMAGE_SIZE\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    do_resize=True,\n",
    "    size={\"max_height\": MAX_SIZE, \"max_width\": MAX_SIZE}\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "model_name = \"microsoft/conditional-detr-resnet-50\"\n",
    "model = ConditionalDetrForObjectDetection.from_pretrained(model_name)\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "image = Image.open(\"../data/KITTI-MOTS/testing/images/0010/000010.png\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = feature_extractor(images=[image], return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([[image.size[1], image.size[0]]])\n",
    "    results = feature_extractor.post_process_object_detection(outputs, threshold=0.7, target_sizes=target_sizes)[0]\n",
    "\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    x, y, x2, y2 = tuple(box)\n",
    "    draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n",
    "    draw.text((x, y), model.config.id2label[label.item()], fill=\"green\")\n",
    "\n",
    "image"
   ],
   "id": "bd54adee00948f51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocess the data",
   "id": "a67841e774cd14a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "MAX_SIZE = IMAGE_SIZE\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    do_resize=True,\n",
    "    size={\"max_height\": MAX_SIZE, \"max_width\": MAX_SIZE}\n",
    ")"
   ],
   "id": "3c8a2dd6756416ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import albumentations as A\n",
    "\n",
    "train_augment_and_transform = A.Compose(\n",
    "    [\n",
    "        A.NoOp()\n",
    "        #A.RandomBrightnessContrast(p=0.5),\n",
    "        #A.HueSaturationValue(p=0.1),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True, min_area=25),\n",
    ")\n",
    "\n",
    "validation_transform = A.Compose(\n",
    "    [A.NoOp()],\n",
    "    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True),\n",
    ")"
   ],
   "id": "18f46c8014fbd542"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def format_image_annotations_as_coco(image_id, categories, areas, bboxes):\n",
    "    \"\"\"Format one set of image annotations to the COCO format\n",
    "\n",
    "    Args:\n",
    "        image_id (str): image id. e.g. \"0001\"\n",
    "        categories (List[int]): list of categories/class labels corresponding to provided bounding boxes\n",
    "        areas (List[float]): list of corresponding areas to provided bounding boxes\n",
    "        bboxes (List[Tuple[float]]): list of bounding boxes provided in COCO format\n",
    "            ([center_x, center_y, width, height] in absolute coordinates)\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"image_id\": image id,\n",
    "            \"annotations\": list of formatted annotations\n",
    "        }\n",
    "    \"\"\"\n",
    "    annotations = []\n",
    "    for category, area, bbox in zip(categories, areas, bboxes):\n",
    "        formatted_annotation = {\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category,\n",
    "            \"iscrowd\": 0,\n",
    "            \"area\": area,\n",
    "            \"bbox\": list(bbox),\n",
    "        }\n",
    "        annotations.append(formatted_annotation)\n",
    "\n",
    "    return {\n",
    "        \"image_id\": image_id,\n",
    "        \"annotations\": annotations,\n",
    "    }"
   ],
   "id": "384f49825eb4ef3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def augment_and_transform_batch(examples, transform, image_processor, return_pixel_mask=False):\n",
    "    \"\"\"Apply augmentations and format annotations in COCO format for object detection task\"\"\"\n",
    "\n",
    "    images = []\n",
    "    annotations = []\n",
    "    for image_id, image, objects in zip(examples[\"image_id\"], examples[\"image\"], examples[\"objects\"]):\n",
    "        try:\n",
    "            image = np.array(Image.open(image).convert(\"RGB\"))\n",
    "\n",
    "            # apply augmentations\n",
    "            output = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n",
    "            images.append(output[\"image\"])\n",
    "\n",
    "            #print(\"Output: \", output)\n",
    "\n",
    "            # format annotations in COCO format\n",
    "            formatted_annotations = format_image_annotations_as_coco(\n",
    "                image_id, output[\"category\"], objects[\"area\"], output[\"bboxes\"]\n",
    "            )\n",
    "            annotations.append(formatted_annotations)\n",
    "            #print(\"Formated annotations:\", formatted_annotations)\n",
    "\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "            # Apply the image processor transformations: resizing, rescaling, normalization\n",
    "        result = image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
    "\n",
    "        if not return_pixel_mask:\n",
    "            result.pop(\"pixel_mask\", None)\n",
    "\n",
    "\n",
    "\n",
    "    return result"
   ],
   "id": "35c075e89f6ddee9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from functools import partial\n",
    "\n",
    "# Make transform functions for batch and apply for dataset splits\n",
    "train_transform_batch = partial(\n",
    "    augment_and_transform_batch, transform=train_augment_and_transform, image_processor=image_processor\n",
    ")\n",
    "validation_transform_batch = partial(\n",
    "    augment_and_transform_batch, transform=validation_transform, image_processor=image_processor\n",
    ")\n",
    "\n",
    "hf_dataset[\"train\"] = hf_dataset[\"train\"].with_transform(train_transform_batch)\n",
    "hf_dataset[\"validation\"] = hf_dataset[\"validation\"].with_transform(validation_transform_batch)\n"
   ],
   "id": "836fd1935af8dc67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    data = {}\n",
    "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
    "    if \"pixel_mask\" in batch[0]:\n",
    "        data[\"pixel_mask\"] = torch.stack([x[\"pixel_mask\"] for x in batch])\n",
    "    return data"
   ],
   "id": "e699aeb1618b4a97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparing function to compute mAP",
   "id": "fd174c7b33089ca7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers.image_transforms import center_to_corners_format\n",
    "\n",
    "def convert_bbox_yolo_to_pascal(boxes, image_size):\n",
    "    \"\"\"\n",
    "    Convert bounding boxes from YOLO format (x_center, y_center, width, height) in range [0, 1]\n",
    "    to Pascal VOC format (x_min, y_min, x_max, y_max) in absolute coordinates.\n",
    "\n",
    "    Args:\n",
    "        boxes (torch.Tensor): Bounding boxes in YOLO format\n",
    "        image_size (Tuple[int, int]): Image size in format (height, width)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Bounding boxes in Pascal VOC format (x_min, y_min, x_max, y_max)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # convert center to corners format\n",
    "    boxes = center_to_corners_format(boxes)\n",
    "\n",
    "    # convert to absolute coordinates\n",
    "    height, width = image_size\n",
    "    boxes = boxes * torch.tensor([[width, height, width, height]])\n",
    "\n",
    "\n",
    "\n",
    "    return boxes"
   ],
   "id": "363a3fff06de351a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    logits: torch.Tensor\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics(evaluation_results, image_processor, threshold=0.0, id2label=None):\n",
    "    \"\"\"\n",
    "    Compute mean average mAP, mAR and their variants for the object detection task.\n",
    "\n",
    "    Args:\n",
    "        evaluation_results (EvalPrediction): Predictions and targets from evaluation.\n",
    "        threshold (float, optional): Threshold to filter predicted boxes by confidence. Defaults to 0.0.\n",
    "        id2label (Optional[dict], optional): Mapping from class id to class name. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Mapping[str, float]: Metrics in a form of dictionary {<metric_name>: <metric_value>}\n",
    "    \"\"\"\n",
    "\n",
    "    predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n",
    "\n",
    "    # For metric computation we need to provide:\n",
    "    #  - targets in a form of list of dictionaries with keys \"boxes\", \"labels\"\n",
    "    #  - predictions in a form of list of dictionaries with keys \"boxes\", \"scores\", \"labels\"\n",
    "\n",
    "    image_sizes = []\n",
    "    post_processed_targets = []\n",
    "    post_processed_predictions = []\n",
    "\n",
    "    # Collect targets in the required format for metric computation\n",
    "    for batch in targets:\n",
    "        # collect image sizes, we will need them for predictions post processing\n",
    "        batch_image_sizes = torch.tensor(np.array([x[\"orig_size\"] for x in batch]))\n",
    "        image_sizes.append(batch_image_sizes)\n",
    "        # collect targets in the required format for metric computation\n",
    "        # boxes were converted to YOLO format needed for model training\n",
    "        # here we will convert them to Pascal VOC format (x_min, y_min, x_max, y_max)\n",
    "        for image_target in batch:\n",
    "            boxes = torch.tensor(image_target[\"boxes\"])\n",
    "            boxes = convert_bbox_yolo_to_pascal(boxes, image_target[\"orig_size\"])\n",
    "            labels = torch.tensor(image_target[\"class_labels\"])\n",
    "            post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
    "\n",
    "    # Collect predictions in the required format for metric computation,\n",
    "    # model produce boxes in YOLO format, then image_processor convert them to Pascal VOC format\n",
    "    for batch, target_sizes in zip(predictions, image_sizes):\n",
    "        batch_logits, batch_boxes = batch[1], batch[2]\n",
    "        output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))\n",
    "        post_processed_output = image_processor.post_process_object_detection(\n",
    "            output, threshold=threshold, target_sizes=target_sizes\n",
    "        )\n",
    "        post_processed_predictions.extend(post_processed_output)\n",
    "\n",
    "    # Compute metrics\n",
    "    metric = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
    "    metric.update(post_processed_predictions, post_processed_targets)\n",
    "    metrics = metric.compute()\n",
    "\n",
    "    # Replace list of per class metrics with separate metric for each class\n",
    "    classes = metrics.pop(\"classes\")\n",
    "    map_per_class = metrics.pop(\"map_per_class\")\n",
    "    mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n",
    "    for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n",
    "        class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n",
    "        metrics[f\"map_{class_name}\"] = class_map\n",
    "        metrics[f\"mar_100_{class_name}\"] = class_mar\n",
    "\n",
    "    metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "eval_compute_metrics_fn = partial(\n",
    "    compute_metrics, image_processor=image_processor, id2label=id2label, threshold=0.0\n",
    ")"
   ],
   "id": "842bcdc188174638"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training the detection model",
   "id": "4f4a4009c96c6fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ],
   "id": "a49b3355585d4ae9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"detr_finetuned_kitti_mots-noaug-good-1\",\n",
    "    num_train_epochs=30,\n",
    "    fp16=False,\n",
    "    per_device_train_batch_size=8,\n",
    "    dataloader_num_workers=4,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=1e-4,\n",
    "    max_grad_norm=0.01,\n",
    "    metric_for_best_model=\"eval_map\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    eval_do_concat_batches=False,\n",
    "    push_to_hub=True\n",
    ")"
   ],
   "id": "7db55bc846111de3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# 2) Create the Trainer (the frozen parameters won't update during training)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_dataset[\"train\"],\n",
    "    eval_dataset=hf_dataset[\"validation\"],\n",
    "    processing_class=image_processor,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=eval_compute_metrics_fn,\n",
    ")\n",
    "\n",
    "# 3) Train, updating only the head\n",
    "trainer.train()"
   ],
   "id": "89d86debcdc5e3e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.push_to_hub()",
   "id": "2eccc0c963fedbd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pprint import pprint\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=hf_dataset[\"validation\"], metric_key_prefix=\"val\")\n",
    "pprint(metrics)"
   ],
   "id": "1505794339fd9dbd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate Validation IoU",
   "id": "b662e10994bf92c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "annotation_file = \"kitti_mots_coco.json\"\n",
    "image_root = \"../data/KITTI-MOTS/training/images\"\n",
    "coco_annotations = load_kitti_mots(annotation_file)\n",
    "\n",
    "# Extraer mapeo de clases (solo se consideran clases 1 y 2)\n",
    "class_mapping, class_names = extract_class_mapping(coco_annotations)\n",
    "print(\"Class mapping:\", class_mapping)\n",
    "print(\"Class names:\", class_names)\n",
    "\n",
    "# Dividir dataset\n",
    "dataset_splits = split_dataset_by_sequence(coco_annotations)\n",
    "\n",
    "# Convertir a Hugging Face `datasets.Dataset`\n",
    "hf_dataset = DatasetDict({\n",
    "    \"train\": create_hf_dataset(dataset_splits[\"train\"], image_root, class_mapping, class_names),\n",
    "    \"validation\": create_hf_dataset(dataset_splits[\"validation\"], image_root, class_mapping, class_names),\n",
    "})"
   ],
   "id": "e968f5e1310a05b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "def center_to_corners_format(boxes):\n",
    "    \"\"\"\n",
    "    Convert boxes from YOLO center format [x_center, y_center, width, height]\n",
    "    to [xmin, ymin, xmax, ymax].\n",
    "    Assumes boxes is a tensor of shape [N, 4].\n",
    "    \"\"\"\n",
    "    x_center = boxes[:, 0]\n",
    "    y_center = boxes[:, 1]\n",
    "    width = boxes[:, 2]\n",
    "    height = boxes[:, 3]\n",
    "    xmin = x_center - width / 2\n",
    "    ymin = y_center - height / 2\n",
    "    xmax = x_center + width / 2\n",
    "    ymax = y_center + height / 2\n",
    "    return torch.stack([xmin, ymin, xmax, ymax], dim=1)\n",
    "\n",
    "def convert_bbox_yolo_to_pascal(boxes, image_size):\n",
    "    \"\"\"\n",
    "    Convert bounding boxes from YOLO format (x_center, y_center, width, height) in [0,1]\n",
    "    to Pascal VOC format (xmin, ymin, xmax, ymax) in absolute coordinates.\n",
    "    \"\"\"\n",
    "    # Convert from center to corners.\n",
    "    boxes = center_to_corners_format(boxes)\n",
    "    height, width = image_size  # image_size in (height, width)\n",
    "    factor = torch.tensor([width, height, width, height], dtype=boxes.dtype, device=boxes.device)\n",
    "    boxes = boxes * factor\n",
    "    return boxes\n",
    "\n",
    "def compute_iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "    Compute Intersection over Union for two boxes.\n",
    "    Boxes are in [xmin, ymin, xmax, ymax] format.\n",
    "    \"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    inter_width = max(0, xB - xA)\n",
    "    inter_height = max(0, yB - yA)\n",
    "    interArea = inter_width * inter_height\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea + 1e-6)\n",
    "    return iou\n",
    "\n",
    "# List to store mean IoU for each sample.\n",
    "sample_ious = []\n",
    "\n",
    "# Loop over the validation set.\n",
    "for sample in tqdm(hf_dataset[\"validation\"], desc=\"Computing IoU on validation set\"):\n",
    "    # 1. Get ground-truth boxes (assumed in COCO format: [xmin, ymin, width, height]) and convert.\n",
    "    gt_boxes = []\n",
    "    for box in sample[\"objects\"][\"bbox\"]:\n",
    "        xmin, ymin, w, h = box\n",
    "        gt_boxes.append([xmin, ymin, xmin + w, ymin + h])\n",
    "    if len(gt_boxes) == 0:\n",
    "        continue  # Skip samples with no ground truth boxes.\n",
    "    gt_boxes = np.array(gt_boxes)\n",
    "\n",
    "    # 2. Run inference on the image.\n",
    "    image_path = sample[\"image\"]  # This should be the full path to the image.\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening image {image_path}: {e}\")\n",
    "        continue\n",
    "    orig_w, orig_h = image.size\n",
    "\n",
    "    # Preprocess the image using your image_processor (which resizes/pads to 480x480).\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use post_process_object_detection to rescale predictions to original image size.\n",
    "    target_sizes = torch.tensor([[orig_h, orig_w]]).to(device)\n",
    "    results = image_processor.post_process_object_detection(\n",
    "        outputs, threshold=0.2, target_sizes=target_sizes\n",
    "    )[0]\n",
    "\n",
    "    pred_boxes = []\n",
    "    # Collect predicted boxes in [xmin, ymin, xmax, ymax] format.\n",
    "    for box in results[\"boxes\"]:\n",
    "        # If your predictions were in YOLO format, you could convert them:\n",
    "        # boxes_tensor = torch.tensor(box).unsqueeze(0)  # shape [1,4]\n",
    "        # box_converted = convert_bbox_yolo_to_pascal(boxes_tensor, (orig_h, orig_w))\n",
    "        # pred_boxes.append(box_converted.squeeze(0).tolist())\n",
    "        # Otherwise, if they are already in [xmin, ymin, xmax, ymax]:\n",
    "        pred_boxes.append(box.tolist())\n",
    "    if len(pred_boxes) == 0:\n",
    "        continue  # Skip sample if no predictions.\n",
    "    pred_boxes = np.array(pred_boxes)\n",
    "\n",
    "    # 3. Compute IoU matrix between ground truth and predicted boxes.\n",
    "    iou_matrix = np.zeros((len(gt_boxes), len(pred_boxes)))\n",
    "    for i, gt in enumerate(gt_boxes):\n",
    "        for j, pred in enumerate(pred_boxes):\n",
    "            iou_matrix[i, j] = compute_iou(gt, pred)\n",
    "\n",
    "    # 4. Use Hungarian matching to maximize total IoU.\n",
    "    # Since linear_sum_assignment minimizes cost, we pass -IoU as the cost.\n",
    "    row_ind, col_ind = linear_sum_assignment(-iou_matrix)\n",
    "    matched_ious = iou_matrix[row_ind, col_ind]\n",
    "\n",
    "    # Average IoU for this sample.\n",
    "    sample_mean_iou = np.mean(matched_ious)\n",
    "    sample_ious.append(sample_mean_iou)\n",
    "\n",
    "# Compute overall mean IoU across samples.\n",
    "if sample_ious:\n",
    "    mean_iou = np.mean(sample_ious)\n",
    "    print(\"Mean IoU over validation set:\", mean_iou)\n",
    "else:\n",
    "    print(\"No samples with both ground truth and predictions to evaluate.\")\n",
    "\n"
   ],
   "id": "140df57f5afe95da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate in Test images",
   "id": "7899ca4416191b87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import AutoModelForObjectDetection\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the feature extractor and DETR model\n",
    "# (Assumes that 'image_processor', 'id2label', and 'label2id' are defined)\n",
    "feature_extractor = image_processor\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    \"toukapy/detr_finetuned_kitti_mots-withaug-3-laura\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# Define base paths for input images and for output images (with drawn detections)\n",
    "input_base = \"../data/KITTI-MOTS/testing/images\"\n",
    "output_base = \"../data/KITTI-MOTS/testing/detecciones\"\n",
    "\n",
    "# Load a default font for drawing text\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "# Recorre la estructura de directorios en input_base\n",
    "for root, dirs, files in os.walk(input_base):\n",
    "    for file in tqdm(files):\n",
    "        if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            # Build full path for current image\n",
    "            input_path = os.path.join(root, file)\n",
    "            # Preserve relative directory structure\n",
    "            rel_dir = os.path.relpath(root, input_base)\n",
    "            out_dir = os.path.join(output_base, rel_dir)\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "            out_file = os.path.join(out_dir, file)\n",
    "\n",
    "            # Open the original image (for final drawing) and get its size\n",
    "            image = Image.open(input_path).convert(\"RGB\")\n",
    "            orig_width, orig_height = image.size\n",
    "\n",
    "            # Resize image to (480, 480) for inference, as in training\n",
    "\n",
    "            # Preprocess the resized image and run inference\n",
    "            inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "            outputs = model(**inputs)\n",
    "            # Set target_sizes to the original image size so boxes are scaled correctly\n",
    "            target_sizes = torch.tensor([[orig_height, orig_width]])\n",
    "            results = feature_extractor.post_process_object_detection(\n",
    "                outputs, threshold=0.3, target_sizes=target_sizes\n",
    "            )[0]\n",
    "\n",
    "            # Prepare to draw on the original image\n",
    "            draw = ImageDraw.Draw(image)\n",
    "\n",
    "            # Loop through detections and draw boxes and labels\n",
    "            for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "                # The output box is now [xmin, ymin, xmax, ymax] in original image coordinates\n",
    "                box = box.tolist()\n",
    "                xmin, ymin, xmax, ymax = map(int, box)\n",
    "                class_name = model.config.id2label[label.item()]\n",
    "                score_val = round(score.item(), 3)\n",
    "                text = f\"{class_name}: {score_val}\"\n",
    "\n",
    "                # Draw bounding box (red rectangle)\n",
    "                draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=2)\n",
    "\n",
    "                # Compute text bounding box using draw.textbbox\n",
    "                text_bbox = draw.textbbox((xmin, ymin), text, font=font)\n",
    "                text_width = text_bbox[2] - text_bbox[0]\n",
    "                text_height = text_bbox[3] - text_bbox[1]\n",
    "\n",
    "                # Draw a filled rectangle behind the text for better visibility\n",
    "                draw.rectangle((xmin, ymin, xmin + text_width, ymin + text_height), fill=\"red\")\n",
    "                # Draw the text in white\n",
    "                draw.text((xmin, ymin), text, fill=\"white\", font=font)\n",
    "\n",
    "            # Save the resulting image with drawn detections\n",
    "            image.save(out_file)"
   ],
   "id": "fe47bd85b03719ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate in Validation images",
   "id": "e0435efe1076da42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import ast\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import AutoModelForObjectDetection\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_base = \"../data/KITTI-MOTS/training/images\"\n",
    "output_base = \"../data/KITTI-MOTS/detect_val_bright_good\"\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "feature_extractor = image_processor\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    \"toukapy/detr_finetuned_kitti_mots-horizon-good-1\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "def load_ground_truth_by_index(idx):\n",
    "    gt_objects = hf_dataset[\"validation\"][\"objects\"][idx]\n",
    "    if isinstance(gt_objects, str):\n",
    "        gt_objects = ast.literal_eval(gt_objects)\n",
    "\n",
    "    gt_list = []\n",
    "    for i in range(len(gt_objects['id'])):\n",
    "        x, y, w, h = gt_objects['bbox'][i]\n",
    "        xmin, ymin, xmax, ymax = x, y, x + w, y + h\n",
    "        label = gt_objects['category'][i]\n",
    "        gt_list.append({\"bbox\": [xmin, ymin, xmax, ymax], \"label\": label})\n",
    "    return gt_list\n",
    "\n",
    "num_images = len(hf_dataset[\"validation\"][\"image\"])\n",
    "for i in range(num_images):\n",
    "    file_name = hf_dataset[\"validation\"][\"image\"][i]\n",
    "    input_path = os.path.join(input_base, file_name)\n",
    "    out_path = output_base + \"/\" + file_name.split(\"/\")[-2] + \"/\" + file_name.split(\"/\")[-1]\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        image = Image.open(input_path).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        print(\"Error al abrir la imagen\", input_path, e)\n",
    "        continue\n",
    "\n",
    "    orig_width, orig_height = image.size\n",
    "\n",
    "    # Image processing and detection\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([[orig_height, orig_width]])\n",
    "    results = feature_extractor.post_process_object_detection(\n",
    "        outputs, threshold=0.3, target_sizes=target_sizes\n",
    "    )[0]\n",
    "\n",
    "    # Draw predictions in red\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        box = box.tolist()\n",
    "        xmin, ymin, xmax, ymax = map(int, box)\n",
    "        class_name = model.config.id2label[label.item()]\n",
    "        score_val = round(score.item(), 3)\n",
    "        text = f\"{class_name}: {score_val}\"\n",
    "        draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=2)\n",
    "        text_bbox = draw.textbbox((xmin, ymin), text, font=font)\n",
    "        text_width = text_bbox[2] - text_bbox[0]\n",
    "        text_height = text_bbox[3] - text_bbox[1]\n",
    "        draw.rectangle((xmin, ymin, xmin + text_width, ymin + text_height), fill=\"red\")\n",
    "        draw.text((xmin, ymin), text, fill=\"white\", font=font)\n",
    "\n",
    "    # Draw ground truth annotations in green\n",
    "    try:\n",
    "        gt_annotations = load_ground_truth_by_index(i)\n",
    "        for gt in gt_annotations:\n",
    "            xmin, ymin, xmax, ymax = map(int, gt[\"bbox\"])\n",
    "            text = f\"GT: {gt['label']}\"\n",
    "            draw.rectangle((xmin, ymin, xmax, ymax), outline=\"green\", width=2)\n",
    "            text_bbox = draw.textbbox((xmin, ymin), text, font=font)\n",
    "            text_width = text_bbox[2] - text_bbox[0]\n",
    "            text_height = text_bbox[3] - text_bbox[1]\n",
    "            draw.rectangle((xmin, ymin, xmin + text_width, ymin + text_height), fill=\"green\")\n",
    "            draw.text((xmin, ymin), text, fill=\"white\", font=font)\n",
    "    except Exception as e:\n",
    "        print(\"Error al procesar ground truth para\", file_name, e)\n",
    "\n",
    "    try:\n",
    "        image.save(out_path)\n",
    "        print(\"Imagen guardada en:\", out_path)\n",
    "    except Exception as e:\n",
    "        print(\"Error al guardar la imagen\", out_path, e)\n"
   ],
   "id": "6d324e369fcb46fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
